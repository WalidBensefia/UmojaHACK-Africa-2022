{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNwjd3IWlgxc"
      },
      "source": [
        "## Table of contents:\n",
        "\n",
        "1. [Importing libraries](#Libraries)\n",
        "2. [Loading data](#Data)\n",
        "3. [Statistical summaries](#Statistics)\n",
        "4. [Missing values and duplicates](#Missing)\n",
        "5. [Outliers](#Outliers)\n",
        "6. [Feature engineering](#Engineering)\n",
        "7. [Date features EDA](#Dates)\n",
        "8. [Correlations - EDA](#Correlations)\n",
        "9. [Preprocess test dataset](#Preprocess)\n",
        "10. [Modelling](#Modelling)\n",
        "11. [Making predictions of the test set and creating a submission file](#Predictions)\n",
        "12. [TO DOs](#Tips)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Kdz7a25j4P"
      },
      "source": [
        "<a name=\"Libraries\"></a>\n",
        "## 1. Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJvGrR3AXhny"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "pd.options.display.float_format = '{:.5f}'.format\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NBDrXUaPsds"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-KRpqn5pbB"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "## 2. Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5QM-Zdx2Ns4g",
        "outputId": "ca18a4da-fb73-4b78-da5d-16794c0a1f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oT0i4Sx7Ni-h"
      },
      "outputs": [],
      "source": [
        "train_path='/content/drive/MyDrive/#UmojaHackAfrica22/train.csv'\n",
        "test_path='/content/drive/MyDrive/#UmojaHackAfrica22/test.csv'\n",
        "samplesubmission_path='/content/drive/MyDrive/#UmojaHackAfrica22/SampleSubmission.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V54N2h-CXzmI",
        "outputId": "494e97e0-fd83-4091-c9d0-94484b5f4348"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bbd4baedf9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msamplesubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamplesubmission_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/#UmojaHackAfrica22/train.csv'"
          ]
        }
      ],
      "source": [
        "# Load files\n",
        "train = pd.read_csv(train_path, parse_dates = ['Datetime'])\n",
        "test = pd.read_csv(test_path, parse_dates = ['Datetime'])\n",
        "samplesubmission = pd.read_csv(samplesubmission_path)\n",
        "\n",
        "# Preview train dataset\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SPMVj-JXXzh-"
      },
      "outputs": [],
      "source": [
        "# Preview test dataset\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V08kByzvZcmU"
      },
      "outputs": [],
      "source": [
        "# Preview sample submission file\n",
        "samplesubmission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2qhol0Yc6Lnt"
      },
      "outputs": [],
      "source": [
        "train[\"difference\"] = abs((train['Sensor1_PM2.5']-train['Sensor2_PM2.5']))\n",
        "train.head()\n",
        "train[train[\"Offset_fault\"]==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V9xqEwnsZEwT"
      },
      "outputs": [],
      "source": [
        "test[\"difference\"] = abs((test['Sensor1_PM2.5']-test['Sensor2_PM2.5']))\n",
        "test.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kdoDJBvdXzf4"
      },
      "outputs": [],
      "source": [
        "# Check size and shape of datasets\n",
        "train.shape, test.shape, samplesubmission.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNtwMcKjZk9x"
      },
      "source": [
        "<a name=\"Statistics\"></a>\n",
        "## 3. Statistical summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S4NqRM3yXzeE"
      },
      "outputs": [],
      "source": [
        "# Train statistical summary\n",
        "train.describe(include = 'all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kESZ-qvnSEa"
      },
      "source": [
        "From the above statistical summary, we can deduce some of the following insights:\n",
        " - The train data provided ranges from *2021-10-15 16:00:34* to *2022-01-21 07:34:57*\n",
        " - There is a high correlation between Sensor1_PM2.5\tand Sensor2_PM2.5\n",
        " - Minimum recorded temperature is *16.70000* and a maximum *34.90000*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2iTls3u3S6Yo"
      },
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(train[\"Offset_fault\"])\n",
        "plt.title('Target variable distribution')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4gly1_uSAeY"
      },
      "source": [
        "The target variable is not well balanced.\n",
        "Some of the techniques to handle imbalanceness include:\n",
        "- Smote\n",
        "- Oversampling\n",
        "- Undersampling ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJNhVNKn5uEE"
      },
      "source": [
        "<a name=\"Missing\"></a>\n",
        "## 4. Missing values and duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3OyFaH-mvx-_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(train.isna())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4Ftu9Y3tbXzk"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "train.isnull().sum().any(), test.isnull().sum().any() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K3eu41vhbXw9"
      },
      "outputs": [],
      "source": [
        "# Plot missing values in train set\n",
        "ax = train.isna().sum().sort_values().plot(kind = 'barh', figsize = (9, 10))\n",
        "plt.title('Percentage of Missing Values Per Column in Train Set', fontdict={'size':15})\n",
        "for p in ax.patches:\n",
        "    percentage ='{:,.0f}%'.format((p.get_width()/train.shape[0])*100)\n",
        "    width, height =p.get_width(),p.get_height()\n",
        "    x=p.get_x()+width+0.02\n",
        "    y=p.get_y()+height/2\n",
        "    ax.annotate(percentage,(x,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7UmbaplH9dVr"
      },
      "outputs": [],
      "source": [
        "print(train.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zftxwl4Ls_p1"
      },
      "outputs": [],
      "source": [
        "test.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cJ5Ck6R63fCh"
      },
      "outputs": [],
      "source": [
        "train['hour']=train.Datetime.dt.hour\n",
        "\n",
        "for column in ['Temperature','Relative_Humidity']:\n",
        "    median = train.groupby('hour')[column].transform('median')\n",
        "\n",
        "    train[column].fillna(median, inplace=True)\n",
        "print(train.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XWXvHjt7Zu9X"
      },
      "outputs": [],
      "source": [
        "train['hour']=train.Datetime.dt.hour\n",
        "\n",
        "for column in ['Temperature','Relative_Humidity']:\n",
        "    median = train.groupby('hour')[column].transform('median')\n",
        "\n",
        "    test[column].fillna(median, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nn1bMy2OzYK3"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(test.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p-ZYjUXE-dWf"
      },
      "outputs": [],
      "source": [
        "train = train.dropna()\n",
        "print(train.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06doaah27vk"
      },
      "source": [
        "Suggestions on how to handle missing values:\n",
        " - Fill in missing values with mode, mean, median..\n",
        " - Drop Missing datapoints with missing values\n",
        " - Fill in with a large number e.g -999999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mALOf0CfbXs0"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "train.duplicated().any(), test.duplicated().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rty_X2Ov4fOy"
      },
      "source": [
        "No duplictes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XWnWEFp5OQTo"
      },
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VmqiddbckaX"
      },
      "source": [
        "<a name=\"Outliers\"></a>\n",
        "## 5. Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERPdXhEx2pjD"
      },
      "source": [
        "Outliers are those data points which differs significantly from other observations present in given dataset.\n",
        "\n",
        "Suggestions on how to handle outliers:\n",
        " - Transforming the outliers by scaling - log transformation, box-cox transformation ...\n",
        " - Dropping outliers\n",
        " - Imputation by replacing outliers with mean, median ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6OwkYAUQ-yzD"
      },
      "outputs": [],
      "source": [
        "print(train.quantile(0.05))\n",
        "print(train.quantile(0.95))\n",
        "num_col = train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "\n",
        "# or you can create a custom list of numerical columns\n",
        "\n",
        "train[num_col]=train[num_col].apply(lambda x: x.clip(*x.quantile([0.05, 0.95])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RRyTGgDU4CcK"
      },
      "outputs": [],
      "source": [
        "num_col=test.select_dtypes(include=['int64','float64']).columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EBGe6nAR3a9C"
      },
      "outputs": [],
      "source": [
        "test[num_col]=test[num_col].clip(train[num_col].quantile(0.05), train[num_col].quantile(0.95), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7IuaUurRdEru"
      },
      "outputs": [],
      "source": [
        "# print(test.quantile(0.05))\n",
        "# print(test.quantile(0.95))\n",
        "# num_col = test.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "\n",
        "# # or you can create a custom list of numerical columns\n",
        "\n",
        "# test[num_col] = test[num_col].apply(lambda x: x.clip(*x.quantile([0.05, 0.95])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cZYjsWux1MBu"
      },
      "outputs": [],
      "source": [
        "train.drop('hour',inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-v4vaXv3bXqh"
      },
      "outputs": [],
      "source": [
        "# Plotting boxplots for each of the numerical columns\n",
        "sns.set_style('darkgrid')\n",
        "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (15, 10))\n",
        "fig.suptitle('Box plots showing outliers', y= 0.93, fontsize = 15)\n",
        "\n",
        "for ax, data, name in zip(axes.flatten(), train, ['Sensor1_PM2.5',\t'Sensor2_PM2.5',\t'Temperature',\t'Relative_Humidity']):\n",
        "  sns.boxplot(train[name], ax = ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndwt2QXVpyXF"
      },
      "source": [
        "<a name=\"Engineering\"></a>\n",
        "## 6. Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pdt5924PbXoY"
      },
      "outputs": [],
      "source": [
        "# Extract day, month year and hour from the Datetime column\n",
        "# day\n",
        "\n",
        "# month\n",
        "train['Datetime_month'] = train.Datetime.dt.month\n",
        " \n",
        "# hour\n",
        "train['Datetime_hour'] = np.sin(train.Datetime.dt.hour*(np.pi/23))\n",
        "\n",
        "\n",
        "# Preview engineered date features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XrbnouNBe8uk"
      },
      "outputs": [],
      "source": [
        "# Extract day, month year and hour from the Datetime column\n",
        "# day\n",
        "\n",
        "# month\n",
        "test['Datetime_month'] = test.Datetime.dt.month\n",
        " \n",
        "# hour\n",
        "test['Datetime_hour'] = np.sin(test.Datetime.dt.hour*(np.pi/23))\n",
        "\n",
        "\n",
        "# Preview engineered date features\n",
        "test[['Datetime', 'Datetime_month', 'Datetime_hour']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY_lY3A2w5Qb"
      },
      "source": [
        "<a name=\"Dates\"></a>\n",
        "## 7. Date features EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6BM9cJepx03e"
      },
      "outputs": [],
      "source": [
        "# Datetime month countplot\n",
        "plt.figure(figsize = (14, 7))\n",
        "sns.countplot(x = 'Datetime_month', data = train)\n",
        "plt.title('Datetime month count plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGIt4f-esd9J"
      },
      "source": [
        "- The only months available in the train set include *Jan, Oct, Nov and Dec*\n",
        "- March has the least number of observations in the dataset while December has the highest number of observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jLifvK1kx8GX"
      },
      "outputs": [],
      "source": [
        "# Box plots for Sensor1_PM2.5\tand Sensor2_PM2.5 vs Offset faults\n",
        "sns.set_style('darkgrid')\n",
        "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (17, 7))\n",
        "fig.suptitle('Box plot for Sensor1_PM2.5\tand Sensor2_PM2.5 vs Offset faults', y= 0.95, fontsize = 15)\n",
        "\n",
        "for ax, data, name in zip(axes.flatten(), train, ['Sensor1_PM2.5', 'Sensor2_PM2.5']):\n",
        "  sns.boxplot(train.Offset_fault, train[name], ax= ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwTKG14O1zo"
      },
      "source": [
        " - Sensor 1 and sensor 2 are highly correlated with majority of the non faulty observations having the highest number of outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9O2HykxLwcJ"
      },
      "source": [
        "<a name=\"Correlations\"></a>\n",
        "## 8. Correlations - EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XvMDQtKDEb0M"
      },
      "outputs": [],
      "source": [
        "# Type of correlations \n",
        "plt.figure(figsize = (20, 12))\n",
        "num_cols = ['Sensor1_PM2.5',\t'Sensor2_PM2.5',\t'Temperature',\t'Relative_Humidity','Datetime_hour']\n",
        "sns.pairplot(train[num_cols], kind=\"scatter\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdwD4SJEPRBB"
      },
      "source": [
        " - There is a positive correlation between Sensor1_PM2.5\tSensor2_PM2.5\n",
        " - There is a negative correlation between temperature and humidity\n",
        " - There seems to be no correlation between sensor PM2.5 and temperature/humidity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hUKo9oVKCQMm"
      },
      "outputs": [],
      "source": [
        "# Quantify correlations\n",
        "corr = train.corr()\n",
        "plt.figure(figsize = (13, 8))\n",
        "sns.heatmap(corr, cmap='RdYlGn', annot = True, center = 0)\n",
        "plt.title('Correlogram', fontsize = 15, color = 'darkgreen')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxifxALcP_p1"
      },
      "source": [
        "- There is a 0.96 positive correlation between sensor1 PM2.5 and sensor2 PM2.5\n",
        "- There is a -0.99 negative correlation between temperature and humidity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PfJUpYgTY02S"
      },
      "outputs": [],
      "source": [
        "train.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KsBYWiWBRCoe"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RAg7LPUKRKJS"
      },
      "outputs": [],
      "source": [
        "# check version number\n",
        "import imblearn\n",
        "print(imblearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XbNecbdqRNED"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nZtrLbk3IQP3"
      },
      "outputs": [],
      "source": [
        "train['avg_sensors']=(train['Sensor1_PM2.5']+train['Sensor2_PM2.5'])/2\n",
        "test['avg_sensors']=(test['Sensor1_PM2.5']+test['Sensor2_PM2.5'])/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ryggTGv8Q3Zm"
      },
      "outputs": [],
      "source": [
        "X=train.drop(['Offset_fault','ID','Datetime','Relative_Humidity'],axis=1)\n",
        "y=train['Offset_fault']\n",
        "X_train,X_test, y_train,y_test=train_test_split(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4yQ2TLW3qFs1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
        "for train,test in sss.split(X,y):\n",
        "  X_train,X_test=X.iloc[train],X.iloc[test]\n",
        "  y_train,y_test=y.iloc[train],y.iloc[test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T0WimgHllIZ3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p-cFzYg7RSms"
      },
      "outputs": [],
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LYN47hEbSe9u"
      },
      "outputs": [],
      "source": [
        "X_test_minmax = min_max_scaler.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "azjAYrDZQ7Zh"
      },
      "outputs": [],
      "source": [
        "oversample = SMOTE()\n",
        "X_train_minmax, y_train = oversample.fit_resample(X_train_minmax, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sXjxZfALYGUE"
      },
      "outputs": [],
      "source": [
        "oversample = SMOTE()\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HcqzrDnulbdi"
      },
      "outputs": [],
      "source": [
        "# (pour entrainer sur toute la BDD Train) X_minmax = min_max_scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r4KlcQCwQ97h"
      },
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(y_train)\n",
        "plt.title('Target variable distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2GYo5nVEmOkl"
      },
      "outputs": [],
      "source": [
        "## (pour entrainer sur toute la BDD Train)\n",
        "#oversample = SMOTE()\n",
        "#X_minmax, y = oversample.fit_resample(X_minmax, y)\n",
        "\n",
        "#sns.set_style('darkgrid')\n",
        "#plt.figure(figsize=(8, 5))\n",
        "#sns.countplot(y)\n",
        "#plt.title('Target variable distribution')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gIDFBaMs8fk"
      },
      "source": [
        "<a name=\"Modelling\"></a>\n",
        "## 10.  Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8IVglhVM9f1B"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# logreg = LogisticRegression()\n",
        "# logreg.fit(X_train_minmax, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z8iKJ95Lmn7J"
      },
      "outputs": [],
      "source": [
        "# neigh = KNeighborsClassifier(11)\n",
        "# neigh.fit(X_train_minmax, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e3cCJUM2s4Q5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Instantiating the model\n",
        "# clf = RandomForestClassifier(max_depth = 3, random_state = 0, n_estimators=200)\n",
        "# clf.fit(X_train_minmax, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AozpttFAFZG"
      },
      "source": [
        "##Fine Tuning GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5B1We0t3XucX"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from scipy.stats import uniform as sp_randFloat\n",
        "from scipy.stats import randint as sp_randInt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wBQIraJw5s3j"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier()\n",
        "gbc.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o4OQW9mY48sY"
      },
      "outputs": [],
      "source": [
        "# Making predictions\n",
        "y_pred = gbc.predict(X_test)\n",
        "\n",
        "# Measuring the accuracy of the model\n",
        "print(f'Accuracy score: {accuracy_score(y_test, y_pred)}')\n",
        "print('\\n')\n",
        "print(f'{classification_report(y_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HBOYZhh0JE9L"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=gbc.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gbc.classes_)\n",
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "disp.plot(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zuFxPcaNHBQM"
      },
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "impo_df = pd.DataFrame({'feature': X.columns, 'importance': neigh.feature_importances_}).set_index('feature').sort_values(by = 'importance', ascending = False)\n",
        "impo_df = impo_df[:12].sort_values(by = 'importance', ascending = True)\n",
        "impo_df.plot(kind = 'barh', figsize = (10, 10), color = 'purple')\n",
        "plt.legend(loc = 'center right')\n",
        "plt.title('Bar chart showing feature importance', color = 'indigo', fontsize = 14)\n",
        "plt.xlabel('Features', fontsize = 12, color = 'indigo')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UuShmECwAbL"
      },
      "source": [
        "<a name=\"Predictions\"></a>\n",
        "## 11. Making predictions of the test set and creating a submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MMOAlt8Vb7Sd"
      },
      "outputs": [],
      "source": [
        "test_df = test.drop(['ID','Datetime','Relative_Humidity'],axis=1)\n",
        "# test_minmax = min_max_scaler.fit_transform(test_df)\n",
        "# test_minmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xJo1lD5cs4MM"
      },
      "outputs": [],
      "source": [
        "# Make prediction on the test set\n",
        "\n",
        "predictions = gbc.predict(test_df)\n",
        "\n",
        "# # Create a submission file\n",
        "sub_file = samplesubmission.copy()\n",
        "sub_file.Offset_fault = predictions\n",
        "\n",
        "# Check the distribution of your predictions\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(sub_file.Offset_fault)\n",
        "plt.title('Predictions Data Distribution');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U_l8khKvcxQZ"
      },
      "outputs": [],
      "source": [
        "sub_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTjjKQchqRVq"
      },
      "source": [
        " - Majority of the model predictions are 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OvzrU6nms4Jw"
      },
      "outputs": [],
      "source": [
        "# Create file\n",
        "sub_file.to_csv('sub_gboost7.csv', index = False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Faulty Air Quality Sensor",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}